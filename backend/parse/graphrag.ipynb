{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669abc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    api_base=\"http://dgx3.humboldt.edu:8000/v1\",\n",
    "    context_window=128000,\n",
    "    api_key=\"2e7f622f5a3c76b9f718205ddd0b860c8b7c6d4fb883b436797a91ee3e0cab3d\",\n",
    "    is_chat_model=True,\n",
    "    is_function_calling_model=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7f4a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec06623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Hello! How can I assist you today?')]), raw=ChatCompletion(id='chatcmpl-JqqLm1DVmed5LEKU04YuxISSb2MS361e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1744522608, model='Qwen/Qwen2.5-32B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='b5121-c94085df', usage=CompletionUsage(completion_tokens=10, prompt_tokens=30, total_tokens=40, completion_tokens_details=None, prompt_tokens_details=None), timings={'prompt_n': 25, 'prompt_ms': 159.516, 'prompt_per_token_ms': 6.38064, 'prompt_per_second': 156.72409037337948, 'predicted_n': 10, 'predicted_ms': 622.519, 'predicted_per_token_ms': 62.2519, 'predicted_per_second': 16.063766728405074}), delta=None, logprobs=None, additional_kwargs={'prompt_tokens': 30, 'completion_tokens': 10, 'total_tokens': 40})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "llm.chat(messages=[ChatMessage(role=\"user\", content=\"Hi\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d9c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "# Note: used to be `Neo4jPGStore`\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=\"neo4j\",\n",
    "    password=\"llamaindex\",\n",
    "    url=\"bolt://localhost:7687\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ab5616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 15\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "2025-04-13 05:36:55,891\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-11_18-21-54_817665_2756939/logs/ray-data\n",
      "2025-04-13 05:36:55,893\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadSQL] -> TaskPoolMapOperator[Project]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running Dataset. Active & requested resources: 1/80 CPU, 333.1MB/74.0GB object store:  13%|█▎        | 825/6.50k [00:01<00:07, 808 row/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Running Dataset. Active & requested resources: 5/80 CPU, 340.6MB/74.0GB object store:  53%|█████▎    | 3.45k/6.50k [00:02<00:01, 1.86k row/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                                                                                                             \n",
      "\u001b[A                                                                                                                                                  \n",
      "\n",
      "✔️  Dataset execution finished in 3.20 seconds: 100%|██████████| 6.45k/6.45k [00:03<00:00, 2.01k row/s]                                   \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A                                                                                                                                                  \n",
      "\n",
      "- ReadSQL->SplitBlocks(2): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 0.0B object store: 100%|██████████| 6.45k/6.45k [00:02<00:00, 2.26k row/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "- Project: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 108.7MB object store: 100%|██████████| 6.45k/6.45k [00:02<00:00, 2.25k row/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import psycopg\n",
    "import ray.data\n",
    "import os\n",
    "from llama_index.core import Document\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "conn_str = os.getenv(\"PG_CONN_STR\")\n",
    "\n",
    "ds = ray.data.read_sql(\"SELECT * FROM documents\", lambda: psycopg.connect(conn_str))\n",
    "\n",
    "documents = ds.select_columns([\"document\"])\n",
    "\n",
    "documents = documents.take_all()\n",
    "\n",
    "documents = [Document.from_dict(json.loads(d['document'])) for d in documents]\n",
    "\n",
    "documents = documents[:50]\n",
    "\n",
    "for document in documents:\n",
    "    document.excluded_llm_metadata_keys = [\"abstract\", \"url\"]\n",
    "    document.excluded_embed_metadata_keys = [\n",
    "        \"abstract\",\n",
    "        \"url\",\n",
    "        \"download_link\",\n",
    "        \"publication_date\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Filter discipline list\n",
    "    discipline = document.metadata.get(\"discipline\")\n",
    "    if discipline is not None:\n",
    "        discipline = list(set(discipline))  # Only unique values\n",
    "        document.metadata[\"discipline\"] = discipline\n",
    "\n",
    "    # Limit number of authors shown in document\n",
    "    author = document.metadata.get(\"author\")\n",
    "    if author is not None:\n",
    "        author = list(set(author))\n",
    "        document.metadata[\"author\"] = author\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "127c7ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 50/50 [00:00<00:00, 209.80it/s]\n"
     ]
    },
    {
     "ename": "APITimeoutError",
     "evalue": "Request timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/http11.py:136\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/http11.py:106\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/http11.py:177\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_async/http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_backends/anyio.py:32\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     26\u001b[0m exc_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ReadTimeout,\n\u001b[1;32m     28\u001b[0m     anyio\u001b[38;5;241m.\u001b[39mBrokenResourceError: ReadError,\n\u001b[1;32m     29\u001b[0m     anyio\u001b[38;5;241m.\u001b[39mClosedResourceError: ReadError,\n\u001b[1;32m     30\u001b[0m     anyio\u001b[38;5;241m.\u001b[39mEndOfStream: ReadError,\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1576\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1576\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1577\u001b[0m         request,\n\u001b[1;32m   1578\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1579\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1580\u001b[0m     )\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_transports/default.py:393\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPITimeoutError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbedding\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperty_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaLLMPathExtractor\n\u001b[0;32m----> 5\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mPropertyGraphIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-mpnet-base-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkg_extractors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSchemaLLMPathExtractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/base.py:119\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    112\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    113\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     transformations,\n\u001b[1;32m    115\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/property_graph/base.py:134\u001b[0m, in \u001b[0;36mPropertyGraphIndex.__init__\u001b[0;34m(self, nodes, llm, kg_extractors, property_graph_store, vector_store, use_async, embed_model, embed_kg_nodes, callback_manager, transformations, storage_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_kg_nodes \u001b[38;5;241m=\u001b[39m embed_kg_nodes\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_vector_store \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    130\u001b[0m     vector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m storage_context\u001b[38;5;241m.\u001b[39mproperty_graph_store\u001b[38;5;241m.\u001b[39msupports_vector_queries\n\u001b[1;32m    132\u001b[0m )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/base.py:77\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 77\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/base.py:185\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **build_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore\u001b[38;5;241m.\u001b[39madd_documents(nodes, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/property_graph/base.py:334\u001b[0m, in \u001b[0;36mPropertyGraphIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **build_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_index_from_nodes\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: Optional[Sequence[BaseNode]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuild_kwargs: Any\n\u001b[1;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexLPG:\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build index from nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# this isn't really used or needed\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IndexLPG()\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/property_graph/base.py:200\u001b[0m, in \u001b[0;36mPropertyGraphIndex._insert_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# run transformations on nodes to extract triplets\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n\u001b[0;32m--> 200\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43marun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kg_extractors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    207\u001b[0m         nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kg_extractors, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py:137\u001b[0m, in \u001b[0;36marun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transform\u001b[38;5;241m.\u001b[39macall(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/property_graph/transformations/schema_llm.py:380\u001b[0m, in \u001b[0;36mSchemaLLMPathExtractor.acall\u001b[0;34m(self, nodes, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m    378\u001b[0m     jobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aextract(node))\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_jobs(\n\u001b[1;32m    381\u001b[0m     jobs,\n\u001b[1;32m    382\u001b[0m     workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m    383\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    384\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting paths from text with schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    385\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/async_utils.py:146\u001b[0m, in \u001b[0;36mrun_jobs\u001b[0;34m(jobs, show_progress, workers, desc)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_asyncio\n\u001b[0;32m--> 146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:571\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/async_utils.py:139\u001b[0m, in \u001b[0;36mrun_jobs.<locals>.worker\u001b[0;34m(job)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;129m@dispatcher\u001b[39m\u001b[38;5;241m.\u001b[39mspan\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mworker\u001b[39m(job: Coroutine) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/indices/property_graph/transformations/schema_llm.py:344\u001b[0m, in \u001b[0;36mSchemaLLMPathExtractor._aextract\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    342\u001b[0m text \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     kg_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mastructured_predict(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkg_schema_cls,\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_prompt,\n\u001b[1;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    348\u001b[0m         max_triplets_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_triplets_per_chunk,\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prune_invalid_triplets(kg_schema)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/llms/openai/base.py:962\u001b[0m, in \u001b[0;36mOpenAI.astructured_predict\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m llm_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_kwargs \u001b[38;5;28;01melse\u001b[39;00m all_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# by default structured prediction uses function calling to extract structured outputs\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# here we force tool_choice to be required\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mastructured_predict(\u001b[38;5;241m*\u001b[39margs, llm_kwargs\u001b[38;5;241m=\u001b[39mllm_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/llms/llm.py:434\u001b[0m, in \u001b[0;36mLLM.astructured_predict\u001b[0;34m(self, output_cls, prompt, llm_kwargs, **prompt_args)\u001b[0m\n\u001b[1;32m    421\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    422\u001b[0m     LLMStructuredPredictStartEvent(\n\u001b[1;32m    423\u001b[0m         output_cls\u001b[38;5;241m=\u001b[39moutput_cls, template\u001b[38;5;241m=\u001b[39mprompt, template_args\u001b[38;5;241m=\u001b[39mprompt_args\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m program \u001b[38;5;241m=\u001b[39m get_program_for_llm(\n\u001b[1;32m    428\u001b[0m     output_cls,\n\u001b[1;32m    429\u001b[0m     prompt,\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    431\u001b[0m     pydantic_program_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_program_mode,\n\u001b[1;32m    432\u001b[0m )\n\u001b[0;32m--> 434\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m program\u001b[38;5;241m.\u001b[39macall(llm_kwargs\u001b[38;5;241m=\u001b[39mllm_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    437\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(LLMStructuredPredictEndEvent(output\u001b[38;5;241m=\u001b[39mresult))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/program/function_program.py:160\u001b[0m, in \u001b[0;36mFunctionCallingProgram.acall\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m llm_kwargs \u001b[38;5;241m=\u001b[39m llm_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    158\u001b[0m tool \u001b[38;5;241m=\u001b[39m get_function_tool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls)\n\u001b[0;32m--> 160\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mapredict_and_call(\n\u001b[1;32m    161\u001b[0m     [tool],\n\u001b[1;32m    162\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39mformat_messages(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    163\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose,\n\u001b[1;32m    164\u001b[0m     allow_parallel_tool_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_parallel_tool_calls,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_kwargs,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tool_outputs(\n\u001b[1;32m    168\u001b[0m     agent_response,\n\u001b[1;32m    169\u001b[0m     allow_parallel_tool_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_parallel_tool_calls,\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/llms/function_calling.py:248\u001b[0m, in \u001b[0;36mFunctionCallingLLM.apredict_and_call\u001b[0;34m(self, tools, user_msg, chat_history, verbose, allow_parallel_tool_calls, error_on_no_tool_call, error_on_tool_error, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_function_calling_model:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapredict_and_call(\n\u001b[1;32m    241\u001b[0m         tools,\n\u001b[1;32m    242\u001b[0m         user_msg\u001b[38;5;241m=\u001b[39muser_msg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    246\u001b[0m     )\n\u001b[0;32m--> 248\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39machat_with_tools(\n\u001b[1;32m    249\u001b[0m     tools,\n\u001b[1;32m    250\u001b[0m     user_msg\u001b[38;5;241m=\u001b[39muser_msg,\n\u001b[1;32m    251\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[1;32m    252\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    253\u001b[0m     allow_parallel_tool_calls\u001b[38;5;241m=\u001b[39mallow_parallel_tool_calls,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    255\u001b[0m )\n\u001b[1;32m    257\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tool_calls_from_response(\n\u001b[1;32m    258\u001b[0m     response, error_on_no_tool_call\u001b[38;5;241m=\u001b[39merror_on_no_tool_call\n\u001b[1;32m    259\u001b[0m )\n\u001b[1;32m    260\u001b[0m tool_tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    261\u001b[0m     acall_tool_with_selection(tool_call, tools, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls\n\u001b[1;32m    263\u001b[0m ]\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/llms/function_calling.py:73\u001b[0m, in \u001b[0;36mFunctionCallingLLM.achat_with_tools\u001b[0;34m(self, tools, user_msg, chat_history, verbose, allow_parallel_tool_calls, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Async chat with function calling.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m chat_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat_with_tools(\n\u001b[1;32m     66\u001b[0m     tools,\n\u001b[1;32m     67\u001b[0m     user_msg\u001b[38;5;241m=\u001b[39muser_msg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39machat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_kwargs)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_chat_with_tools_response(\n\u001b[1;32m     75\u001b[0m     response,\n\u001b[1;32m     76\u001b[0m     tools,\n\u001b[1;32m     77\u001b[0m     allow_parallel_tool_calls\u001b[38;5;241m=\u001b[39mallow_parallel_tool_calls,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     79\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/llms/openai_like/base.py:158\u001b[0m, in \u001b[0;36mOpenAILike.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     completion_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macomplete(prompt, formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion_response_to_chat_response(completion_response)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39machat(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    361\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    362\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:75\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     67\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     68\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     },\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     77\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m     78\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     79\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m     80\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m     81\u001b[0m     )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/llms/openai/base.py:633\u001b[0m, in \u001b[0;36mOpenAI.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     achat_fn \u001b[38;5;241m=\u001b[39m acompletion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acomplete)\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m achat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:189\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    188\u001b[0m async_wrapped\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:111\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:114\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/llama_index/llms/openai/base.py:683\u001b[0m, in \u001b[0;36mOpenAI._achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(\n\u001b[1;32m    678\u001b[0m     messages,\n\u001b[1;32m    679\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreuse_client:\n\u001b[0;32m--> 683\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m aclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    684\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessage_dicts, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclient:\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/resources/chat/completions.py:1661\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1659\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1660\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1663\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1664\u001b[0m             {\n\u001b[1;32m   1665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1694\u001b[0m             },\n\u001b[1;32m   1695\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1696\u001b[0m         ),\n\u001b[1;32m   1697\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1698\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1699\u001b[0m         ),\n\u001b[1;32m   1700\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1701\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1702\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1703\u001b[0m     )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1831\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1839\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1840\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1841\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1842\u001b[0m     )\n\u001b[0;32m-> 1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1543\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1585\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1582\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m         input_options,\n\u001b[1;32m   1587\u001b[0m         cast_to,\n\u001b[1;32m   1588\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1589\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1590\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1591\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1676\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1585\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1582\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m         input_options,\n\u001b[1;32m   1587\u001b[0m         cast_to,\n\u001b[1;32m   1588\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1589\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1590\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1591\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1676\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1585\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1582\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m         input_options,\n\u001b[1;32m   1587\u001b[0m         cast_to,\n\u001b[1;32m   1588\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1589\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1590\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1591\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1676\u001b[0m )\n",
      "File \u001b[0;32m~/vllm-env/lib/python3.10/site-packages/openai/_base_client.py:1595\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m             input_options,\n\u001b[1;32m   1587\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1592\u001b[0m         )\n\u001b[1;32m   1594\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1597\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAPITimeoutError\u001b[0m: Request timed out."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.8867944117578586 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.83584521926418 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3843145349460317 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9007317870150806 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.4590575441825888 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.687996750617765 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.016882698297858 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0538435091113967 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3063686153710206 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3162875169143444 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5314541976513554 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7892959887969153 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.036203380620427 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.270857635717658 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2098071671021113 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9684724286060107 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.269006005930493 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9979488408863726 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1268110206627306 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.273371979457397 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1875056970569497 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1616292605831353 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1413107754312102 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.577561433238249 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.8060470607169574 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3464763725774076 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3494930625292012 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7156791375502418 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.506321404393693 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2599076847391522 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.4490172378292376 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.173649779597633 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9436537306947304 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2087000355310287 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7847803575331684 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.292556202314731 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6952723471749234 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7788342209016537 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7335605447642353 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2308386889024354 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7903883148254698 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3813463457644557 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2150077773708732 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.4794332650923956 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7524907274557289 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7659745701774288 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7539485903221488 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.687576621667455 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0365448968621431 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0511291365591002 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.4383217140772144 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.269618112156892 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9942904202209566 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6414004263246427 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.30318891458953 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9269775745745643 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.125075802836658 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.626143219898574 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5417696114387103 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6284438336780123 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7739176986867866 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.160995785444642 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0173292993580143 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7314661981948505 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1728444338683102 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0199994715048162 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5227364036323516 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7209134341150523 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3219612818302058 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0713817795536111 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.265738432280508 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.576668479088045 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6046690190371204 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9609533174381957 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5507730006428964 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3643862746404292 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9518121791064302 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.8204806923326173 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2825806717557409 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2044915596801546 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9859551803997095 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7800629038394997 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.972118054982055 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1528049828590672 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3480544115646045 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5306048657192364 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9583759945913153 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.1618913601022485 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0887475890751979 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2550827272138556 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6976817707454597 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.3580770236361546 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2837298573602278 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5499422711555833 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2692370044260566 seconds as it raised APIConnectionError: Connection error..\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=HuggingFaceEmbedding(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    ),\n",
    "    kg_extractors=[\n",
    "        SchemaLLMPathExtractor(\n",
    "            llm=llm\n",
    "        )\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b919be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
